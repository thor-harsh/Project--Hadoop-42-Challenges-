# Project--Hadoop-42-Challenges-

<table>
  
**In this project We will use Spark with Python to do an amazing stuff.Here we will work on the Spark DataFrame which will read the csv files attached above as our dataset and complete the challenges as provided by Jose Portilla .** <br></br>

**Here is the Problem Statement!** <br></br>

A large technology firm needs your help, they've been hacked! Luckily their forensic engineers have grabbed valuable data about the hacks, including information like session time,locations, wpm typing speed, etc. The forensic engineer relates to you what she has been able to figure out so far, she has been able to grab meta data of each session that the hackers used to connect to their servers. <br></br> These are the features of the data:<br></br>


'Session_Connection_Time': How long the session lasted in minutes <br></br>
'Bytes Transferred': Number of MB transferred during session <br></br>
'Kali_Trace_Used': Indicates if the hacker was using Kali Linux <br></br>
'Servers_Corrupted': Number of server corrupted during the attack <br></br>
'Pages_Corrupted': Number of pages illegally accessed <br></br>
'Location': Location attack came from (Probably useless because the hackers used VPNs) <br></br>
'WPM_Typing_Speed': Their estimated typing speed based on session logs. <br></br>

The technology firm has 3 potential hackers that perpetrated the attack. Their certain of the first two hackers but they aren't very sure if the third hacker was involved or not. They have requested your help! Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just two hackers? It's probably not possible to know for sure, but maybe what you've just learned about Clustering can help!<br></br>

**One last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers.** <br></br>


**Before jumping to the code lets understand Spark and Random Forest First**...<br></br>

**What is Apache Spark?** <br></br>

Apache Spark‚Ñ¢ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.<br></br>
Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R.
Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses.<br></br>
Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling.
Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.<br></br>

**What is Spark DataFrames**?<br></br>

**1**: Spark 2.0 shifted towards DataFrame syntax<br></br>
**2**: are now the standard way of using Spark's ML Capabilties<br></br>
**3**: Spark Docs are still new<br></br>
**4**: DataFrame is very familiar to Pandas DataFrames<br></br>
**5**: Columns = features<br></br>
**6**: Rows = records<br></br>

**What is Random Forest**?<br></br>
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees.<br></br>


**Important Note: Go through the customer_churn.csv and new_customers.csv files before jumping to the code.**


</table>

**So what are you waiting for...? Jump to the code to get started. As usual for any doubt or query see you in pull request section üòÅüòÇ. Thanks!**


